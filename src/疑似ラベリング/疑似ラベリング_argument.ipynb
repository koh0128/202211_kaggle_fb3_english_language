{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"fKPbm_SwWmZD"},"outputs":[],"source":["import sys\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bPGxPgYRnO8B"},"outputs":[],"source":["COMPETION_DATA_DIR = '/content/drive/MyDrive/input/kaggle/feedback-prize-english-language-learning'\n","iter_path = COMPETION_DATA_DIR + '/iterativestratification'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iiIpq4HnZTMx"},"outputs":[],"source":["!pip install tokenizers\n","!pip install transformers\n","!pip install iterstrat\n","!pip install sentencepiece"]},{"cell_type":"code","source":["pip install --upgrade pip"],"metadata":{"id":"p-dlyA3SwE48"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tIaFw6BwYJne"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import os\n","import gc\n","import re\n","import ast\n","import copy\n","import json\n","import time\n","import math\n","import string\n","import pickle\n","import random\n","import joblib\n","import itertools\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","import scipy as sp\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","from matplotlib import pyplot as plt\n","pd.set_option('display.max_rows', 500)\n","pd.set_option('display.max_columns', 500)\n","pd.set_option('display.width', 1000)\n","from tqdm.auto import tqdm\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","sys.path.append(iter_path)\n","from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n","import torch\n","import torch.nn as nn\n","from torch.nn import Parameter\n","import torch.nn.functional as F\n","from torch.optim import Adam, SGD, AdamW\n","from torch.utils.data import DataLoader, Dataset\n","from torch.utils import checkpoint\n","import tokenizers\n","import transformers\n","from transformers import AutoTokenizer, AutoModel, AutoConfig\n","from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n","%env TOKENIZERS_PARALLELISM=true\n","%matplotlib inline\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","import sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FL-gUtckYQRC"},"outputs":[],"source":["class CFG:\n","    eda = False\n","    debug = False\n","    load_bert_data = False\n","    load_test_only = False\n","    model_train = False\n","    train_multi = True\n","    train_staking = False\n","    train_ensumbling = True\n","    headrow = 3\n","    num_workers=4\n","    gradient_checkpointing=True\n","    scheduler='cosine' # ['linear', 'cosine']\n","    batch_scheduler=True\n","    num_cycles=0.5\n","    num_warmup_steps=0\n","    epochs=4\n","    encoder_lr=2e-5\n","    decoder_lr=2e-5\n","    min_lr=1e-6\n","    eps=1e-6\n","    betas=(0.9, 0.999)\n","    batch_size=8\n","    max_len=512\n","    weight_decay=0.01\n","    gradient_accumulation_steps=1\n","    max_grad_norm=1000\n","    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n","    seed=42\n","    n_fold=25\n","    train=True\n","    \n","if CFG.debug:\n","    CFG.n_fold = 5\n","    \n","if CFG.train_staking == False & CFG.train_ensumbling == False:\n","    CFG.train_ensumbling = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VUi1iK3TnO1C"},"outputs":[],"source":["target_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions',]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t5_qt4AbYQW1"},"outputs":[],"source":["# def get_logger(filename=OUTPUT_DIR + 'train'):\n","#     from logging import getLogger, INFO, FileHandler, Formatter, StreamHandler\n","#     logger = getLogger(__name__)\n","#     logger.setLevel(INFO)\n","#     handler1 = StreamHandler()\n","#     handler1.setFormatter(Formatter(\"%(message)s\"))\n","#     handler2 = FileHandler(filename=f\"{filename}.log\")\n","#     handler2.setFormatter(Formatter(\"%(message)s\"))\n","#     logger.addHandler(handler1)\n","#     logger.addHandler(handler2)\n","#     return logger\n","\n","\n","# LOGGER = get_logger()\n","\n","\n","def seed_everything(seed=CFG.seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","\n","seed_everything(seed=42)\n","\n","\n","def get_essay(essay_id, is_train=True):\n","    parent_path = INPUT_DIR + 'train' if is_train else INPUT_DIR + 'test'\n","\n","    try:\n","        essay_path = os.path.join(parent_path, f\"{essay_id}.txt\")\n","        essay_text = open(essay_path, 'r').read()\n","        return essay_text\n","    except:\n","        return ''\n","\n","def criterion_val(outputs, labels):\n","    return nn.CrossEntropyLoss()(outputs, labels)\n","\n","\n","def criterion_train(outputs, labels):\n","    return nn.CrossEntropyLoss()(outputs, labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"osraiAEQmmm7"},"outputs":[],"source":["from transformers import AutoModel,AutoTokenizer\n","import torch\n","import torch.nn.functional as F\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wr49Q_62mo3P"},"outputs":[],"source":["def mean_pooling(model_output, attention_mask):\n","    token_embeddings = model_output.last_hidden_state.detach().cpu()\n","    input_mask_expanded = (\n","        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","    )\n","    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n","        input_mask_expanded.sum(1), min=1e-9\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Q3M0b4kmrXC"},"outputs":[],"source":["BATCH_SIZE = 4\n","\n","class EmbedDataset(torch.utils.data.Dataset):\n","    def __init__(self,df):\n","        self.df = df.reset_index(drop=True)\n","    def __len__(self):\n","        return len(self.df)\n","    def __getitem__(self,idx):\n","        text = self.df.loc[idx,\"full_text\"]\n","        tokens = tokenizer(\n","                text,\n","                None,\n","                add_special_tokens=True,\n","                padding='max_length',\n","                truncation=True,\n","                max_length=MAX_LEN,return_tensors=\"pt\")\n","        tokens = {k:v.squeeze(0) for k,v in tokens.items()}\n","        return tokens\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gGJAiWQ5mrdK"},"outputs":[],"source":["tokenizer = None\n","MAX_LEN = 640\n","\n","def get_embeddings(MODEL_NM='', MAX=640, BATCH_SIZE=4, verbose=True):\n","    global tokenizer, MAX_LEN\n","    DEVICE=\"cuda\"\n","    model = AutoModel.from_pretrained( MODEL_NM )\n","    tokenizer = AutoTokenizer.from_pretrained( MODEL_NM )\n","    MAX_LEN = MAX\n","    \n","    model = model.to(DEVICE)\n","    model.eval()\n","    all_train_text_feats = []\n","    for batch in tqdm(embed_dataloader_tr,total=len(embed_dataloader_tr)):\n","        input_ids = batch[\"input_ids\"].to(DEVICE)\n","        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n","        with torch.no_grad():\n","            model_output = model(input_ids=input_ids,attention_mask=attention_mask)\n","        sentence_embeddings = mean_pooling(model_output, attention_mask.detach().cpu())\n","        # Normalize the embeddings\n","        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n","        sentence_embeddings =  sentence_embeddings.squeeze(0).detach().cpu().numpy()\n","        all_train_text_feats.extend(sentence_embeddings)\n","    all_train_text_feats = np.array(all_train_text_feats)\n","    if verbose:\n","        print('Train embeddings shape',all_train_text_feats.shape)\n","\n","        \n","    return all_train_text_feats"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pOhm9poFmrpl"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Elyof0sYSGo"},"outputs":[],"source":["INPUT_DIR = '/content/drive/MyDrive/input/kaggle/feedback-prize-effectiveness/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PknsphEGbuGa"},"outputs":[],"source":["train = pd.read_csv(INPUT_DIR + 'train.csv')\n","if CFG.debug:\n","    train = train[:1000]\n","test = pd.read_csv(INPUT_DIR + 'test.csv')\n","print(train.head())\n","print(train.shape)\n","print(test.head())\n","print(test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FzHLLsr0YSPV"},"outputs":[],"source":["pseudo_train = train['essay_id'].drop_duplicates().reset_index(drop = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rbQknM9vYSWM"},"outputs":[],"source":["pseudo_train['full_text'] = pseudo_train['essay_id'].apply(lambda x: get_essay(x, is_train=True))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XC78tUBnfklm"},"outputs":[],"source":["pseudo_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b9zn5soJfzZo"},"outputs":[],"source":["!pip install readability"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LuqUm6CSVHYP"},"outputs":[],"source":["import readability\n","import pandas as pd\n","import re\n","import seaborn as sns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"32vaAfg5VWnJ"},"outputs":[],"source":["def data_cleaner(text):\n","    text = text.strip()\n","    text = re.sub(r'\\n', '', text)\n","    text = text.lower()\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mn_Jl7acVWtN"},"outputs":[],"source":["def calc_readability(text):\n","\n","    results = readability.getmeasures(text, lang='en')\n","\n","    return results['readability grades']['ARI']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aCmxA2nYfpPU"},"outputs":[],"source":["pseudo_train['full_text'] = pseudo_train['full_text'].apply(data_cleaner)\n","pseudo_train['readability'] = pseudo_train['full_text'].apply(calc_readability)\n","pseudo_train['words'] = pseudo_train['full_text'].apply(lambda x: len(x.split()))"]},{"cell_type":"code","source":["pseudo_train.head()"],"metadata":{"id":"DICyK-ziJ0oa"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-z7oSFCVfpUW"},"outputs":[],"source":["sns.pairplot(pseudo_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZaINzKtCgPUr"},"outputs":[],"source":["min_readability = 92.793281\n","max_readability = 396.656655\n","min_words = 185.000000\n","max_words = 783.000000"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KNyHARpNgPPF"},"outputs":[],"source":["pseudo_train = pseudo_train[\n","    (pseudo_train['readability'] >= min_readability)\n","    &(pseudo_train['readability'] <= max_readability)\n","    &(pseudo_train['words'] >= min_words)\n","    &(pseudo_train['words'] <= max_words)\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xz4YfXfjgPKA"},"outputs":[],"source":["pseudo_train.shape"]},{"cell_type":"code","source":[],"metadata":{"id":"svrR_vdpegtN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6YnXj6JnegzM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"59WKegbBeg5P"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qQ2-zsGtheBD"},"outputs":[],"source":["ds_tr = EmbedDataset(pseudo_train)\n","embed_dataloader_tr = torch.utils.data.DataLoader(ds_tr,\\\n","                        batch_size=BATCH_SIZE,\\\n","                        shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ta8vO7xyheGH"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IMoa1ybFmrj3"},"outputs":[],"source":["MODEL_NM = COMPETION_DATA_DIR + '/huggingface-deberta-variants/deberta-base/deberta-base'\n","pseudo_train_text_feats = get_embeddings(MODEL_NM)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yAoQKEx5heKz"},"outputs":[],"source":["MODEL_NM =  COMPETION_DATA_DIR + '/deberta-v3-large/deberta-v3-large'\n","pseudo_train_text_feats2 = get_embeddings(MODEL_NM)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FUVYoPoRhePA"},"outputs":[],"source":["MODEL_NM = COMPETION_DATA_DIR +  '/huggingface-deberta-variants/deberta-large/deberta-large'\n","pseudo_train_text_feats3 = get_embeddings(MODEL_NM)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uFjNH530heS5"},"outputs":[],"source":["MODEL_NM = COMPETION_DATA_DIR + '/huggingface-deberta-variants/deberta-large-mnli/deberta-large-mnli'\n","pseudo_train_text_feats4 = get_embeddings(MODEL_NM, MAX=512)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C6gJ2CY8heW8"},"outputs":[],"source":["MODEL_NM = COMPETION_DATA_DIR + '/huggingface-deberta-variants/deberta-xlarge/deberta-xlarge'\n","pseudo_train_text_feats5 = get_embeddings(MODEL_NM, MAX=512)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U430bT7cVWy2"},"outputs":[],"source":["pseudo_train_text_feats = np.concatenate([\n","    pseudo_train_text_feats,\n","    pseudo_train_text_feats2,\n","    pseudo_train_text_feats3,\n","    pseudo_train_text_feats4,\n","    pseudo_train_text_feats5\n","],\n","    axis=1\n",")\n","\n","del pseudo_train_text_feats2\n","del pseudo_train_text_feats3\n","del pseudo_train_text_feats4\n","del pseudo_train_text_feats5\n","gc.collect()\n","\n","print('Our concatenated embeddings have shape', pseudo_train_text_feats.shape )"]},{"cell_type":"code","source":["# all_train_text_feats = np.load(npy_path + '/all_train_text_feats.npy')"],"metadata":{"id":"ycMxJgs1r--N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.linear_model import Ridge\n","from sklearn.linear_model import Lasso\n","from sklearn.linear_model import LinearRegression as LR\n","from sklearn.ensemble import RandomForestRegressor as RFR\n","from sklearn.svm import SVR"],"metadata":{"id":"e_EP2A3sr_Fl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if CFG.train_multi == True:\n","    models = {\n","        'SVR(C=1)':SVR(C=1), \n","        'Ridge(alpha=1)':Ridge(alpha=1), \n","        # 'Lasso(alpha=1)':Lasso(alpha=1), \n","        'RFR(max_depth=5, n_estimators=10)':RFR(max_depth=5, n_estimators=10)\n","    }\n","else:\n","    models = {\n","        'Ridge(alpha=1)':Ridge(alpha=1), \n","    }"],"metadata":{"id":"xikQ_v5Rr_LN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["FOLDS = CFG.n_fold"],"metadata":{"id":"pwORFlvWHj__"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.save('/content/drive/MyDrive/input/kaggle/feedback-prize-effectiveness/pseudo_train_text_feats', pseudo_train_text_feats)"],"metadata":{"id":"6TAiVBSwJVV0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import mean_squared_error\n","\n","preds = []\n","scores = []\n","stacks = []\n","def comp_score(y_true,y_pred):\n","    rmse_scores = []\n","    for i in range(len(target_cols)):\n","        rmse_scores.append(np.sqrt(mean_squared_error(y_true[:,i],y_pred[:,i])))\n","    return np.mean(rmse_scores)\n","\n","for model in models.keys():\n","\n","    #for fold in tqdm(range(FOLDS),total=FOLDS):\n","    for fold in range(FOLDS):\n","        print('#'*25)\n","        print('### Fold',fold)\n","        print('#'*25)\n","        \n","        # dftr_ = dftr[dftr[\"FOLD\"]!=fold]\n","        # dfev_ = dftr[dftr[\"FOLD\"]==fold]\n","        \n","        # tr_text_feats = all_train_text_feats[list(dftr_.index),:]\n","        # ev_text_feats = all_train_text_feats[list(dfev_.index),:]\n","        \n","        # ev_preds = np.zeros((len(ev_text_feats),6))\n","        test_preds = np.zeros((len(pseudo_train_text_feats),6))\n","        # stack_preds = np.zeros((len(all_train_text_feats[list(dftr.index),:]),6))\n","        \n","        for i,t in enumerate(target_cols):\n","\n","            if CFG.model_train == True:\n","                pass\n","                # print(t,', ',end='')\n","                # clf = models[model]\n","                # clf.fit(tr_text_feats, dftr_[t].values)\n","                # ev_preds[:,i] = clf.predict(ev_text_feats)\n","                # test_preds[:,i] = clf.predict(te_text_feats)\n","                # stack_preds[:,i] = clf.predict(all_train_text_feats[list(dftr.index),:])\n","\n","                # # モデルを保存する\n","                # if 'google.colab' in sys.modules:\n","                #     filename = f'{COMPETION_DATA_DIR}/models/finalized_{model}_{t}_{fold}.sav'\n","                # elif 'kaggle_web_client' in sys.modules:\n","                #     filename = f'finalized_{model}_{t}_{fold}.sav'\n","                # pickle.dump(clf, open(filename, 'wb'))\n","\n","            else:\n","     \n","                filename = f'{COMPETION_DATA_DIR}/models/finalized_{model}_{t}_{fold}.sav'\n","                clf = pickle.load(open(filename, 'rb'))\n","                # ev_preds[:,i] = clf.predict(ev_text_feats)\n","                test_preds[:,i] = clf.predict(pseudo_train_text_feats)\n","                # stack_preds[:,i] = clf.predict(all_train_text_feats[list(dftr.index),:])\n","            \n","            gc.collect()\n","\n","        print()\n","        # score = comp_score(dfev_[target_cols].values,ev_preds)\n","        # scores.append(score)\n","        \n","        # stacks.append(stack_preds)\n","        \n","        # print(\"Fold : {} RSME score: {}\".format(fold,score))\n","        preds.append(test_preds)\n","    \n","print('#'*25)\n","print('Overall CV RSME =',np.mean(scores))"],"metadata":{"id":"x7wN9Dcor_QJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if CFG.train_ensumbling == True:\n","    sub = dfte.copy()\n","\n","    for i,t in enumerate(target_cols): \n","\n","        ensumbe_final_pred = pd.DataFrame()\n","        ensumbe_final_pred_mean = pd.DataFrame()\n","\n","        for ii in range(len(stacks)):\n","            ensumbe_final_pred = pd.concat([ensumbe_final_pred, pd.DataFrame(preds[ii], columns = target_cols)[t]], axis = 1)\n","\n","        for jj in range(1, len(models)+1):\n","            ensumbe_final_pred_mean = pd.concat([ensumbe_final_pred_mean, ensumbe_final_pred.iloc[:, CFG.n_fold * (jj-1): CFG.n_fold * (jj)].mean(axis = 1)], axis = 1)\n","\n","        if ensumbe_final_pred_mean.shape[1] >= 2:\n","            sub[t] = np.mean(ensumbe_final_pred_mean, axis=1)\n","        else:\n","            sub[t] = ensumbe_final_pred_mean\n","\n","    sub_columns = pd.read_csv(sub_path).columns\n","    sub = sub[sub_columns]\n","    sub.head()"],"metadata":{"id":"-iJ4LllcsfIa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Zkkr98ODsfP7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YB9OldcSsfWu"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyNZVRolEOUMb0wYY6isPbwK"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}